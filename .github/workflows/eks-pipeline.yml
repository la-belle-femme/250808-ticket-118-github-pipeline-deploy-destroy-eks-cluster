name: EKS Cluster Management

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Select action'
        required: true
        default: 'plan'
        type: choice
        options:
          - plan
          - apply
          - destroy

env:
  CLUSTER_NAME: github-action-eks-cluster
  AWS_REGION: us-east-1

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-${{ github.run_id }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Terraform Init
        run: terraform init -input=false

      - name: Terraform Validate
        run: terraform validate

      - name: Terraform Plan
        if: github.event.inputs.action == 'plan'
        run: terraform plan -input=false

      - name: Terraform Apply
        if: github.event.inputs.action == 'apply'
        run: terraform apply -auto-approve -input=false

      - name: Destroy EKS Cluster
        if: github.event.inputs.action == 'destroy'
        timeout-minutes: 45  # Fixed timeout as number, not variable
        run: |
          #!/bin/bash
          set -eo pipefail

          function cleanup_resources() {
            echo "::group::1. Deleting Node Groups"
            nodegroups=$(aws eks list-nodegroups \
              --cluster-name "$CLUSTER_NAME" \
              --region "$AWS_REGION" \
              --query "nodegroups" \
              --output text || echo "")
            
            if [ -n "$nodegroups" ]; then
              for ng in $nodegroups; do
                echo "Scaling down nodegroup: $ng"
                aws eks update-nodegroup-config \
                  --cluster-name "$CLUSTER_NAME" \
                  --nodegroup-name "$ng" \
                  --region "$AWS_REGION" \
                  --scaling-config minSize=0,maxSize=0,desiredSize=0 || true
                
                echo "Deleting nodegroup: $ng"
                aws eks delete-nodegroup \
                  --cluster-name "$CLUSTER_NAME" \
                  --nodegroup-name "$ng" \
                  --region "$AWS_REGION" || true
                
                echo "Waiting for deletion (max 10 minutes)..."
                timeout 600 aws eks wait nodegroup-deleted \
                  --cluster-name "$CLUSTER_NAME" \
                  --nodegroup-name "$ng" \
                  --region "$AWS_REGION" || true
              done
            fi
            echo "::endgroup::"

            echo "::group::2. Deleting Fargate Profiles"
            profiles=$(aws eks list-fargate-profiles \
              --cluster-name "$CLUSTER_NAME" \
              --region "$AWS_REGION" \
              --query "fargateProfileNames" \
              --output text || echo "")
            
            for profile in $profiles; do
              aws eks delete-fargate-profile \
                --cluster-name "$CLUSTER_NAME" \
                --fargate-profile-name "$profile" \
                --region "$AWS_REGION" || true
            done
            echo "::endgroup::"

            echo "::group::3. Running Terraform Destroy"
            terraform destroy -auto-approve -input=false -refresh=false
            echo "::endgroup::"

            echo "::group::4. Cleaning Orphaned Resources"
            # Cleanup CloudFormation stacks
            stacks=$(aws cloudformation list-stacks \
              --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE \
              --query "StackSummaries[?contains(StackName, '$CLUSTER_NAME')].StackName" \
              --output text --region "$AWS_REGION" 2>/dev/null || echo "")
            
            for stack in $stacks; do
              aws cloudformation delete-stack \
                --stack-name "$stack" \
                --region "$AWS_REGION" || true
            done

            # Cleanup Load Balancers
            lbs=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, '$CLUSTER_NAME')].LoadBalancerArn" \
              --output text --region "$AWS_REGION" 2>/dev/null || echo "")
            
            for lb in $lbs; do
              aws elbv2 delete-load-balancer \
                --load-balancer-arn "$lb" \
                --region "$AWS_REGION" || true
            done
            echo "::endgroup::"
          }

          # Main execution
          if ! aws eks describe-cluster \
            --name "$CLUSTER_NAME" \
            --region "$AWS_REGION" &>/dev/null; then
            echo "::warning::Cluster $CLUSTER_NAME not found"
            exit 0
          fi

          cleanup_resources

          echo "::group::5. Final Verification"
          if aws eks describe-cluster \
            --name "$CLUSTER_NAME" \
            --region "$AWS_REGION" &>/dev/null; then
            echo "::error::Cluster $CLUSTER_NAME still exists after destruction!"
            exit 1
          else
            echo "âœ… Cluster $CLUSTER_NAME successfully destroyed"
          fi
          echo "::endgroup::"
